{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analyst Nanodegree - Intro to Machine Learning - Final Project P5\n",
    "by Vitor Bellini\n",
    "## Enron Submission Free-Response Questions\n",
    "\n",
    "![enron_bankrupt](imgs/enron_bankrupt.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "**Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The Enron scandal was a bankrupt of one of the biggest United Stated companies due to frauds involving large part of the corporation employees. The goal of this project is to identify Persons of Interest (POI) based on data about the e-mails sent/received of Enron employees and data about their finances, such as salary, bonus, stocks, etc. A Person of Interest is an employee that participated on Enron scandal and was investigated in this fraud case, which means that they have been indicted, entered into agreements with the government, or have testified in exchange for immunity in the case._\n",
    "\n",
    "_Machine learning algorithms are very suited to this task. With a set of input features (emails and finances) and the knowledge of wich of the employees were later considered POI (labels), we can use ML to try to predict who would be an POI based on emails and financial information._\n",
    "\n",
    "**The dataset**\n",
    "\n",
    "_The original dataset contained information about 146 data points (Enron employees), with 18 POIs and 128 non-POIs. The total features number is 21, with 14 financial, 6 email and 1 with the POI label._\n",
    "\n",
    "**Missing information**\n",
    "\n",
    "_All of the features, except the POI label has at least 20 NaN values. Here's the number of NaN by feature:_\n",
    "\n",
    "[(142, 'loan_advances'), <br>\n",
    " (129, 'director_fees'),<br>\n",
    " (128, 'restricted_stock_deferred'),<br>\n",
    " (107, 'deferral_payments'),<br>\n",
    " (97, 'deferred_income'),<br>\n",
    " (80, 'long_term_incentive'),<br>\n",
    " (64, 'bonus'),<br>\n",
    " (60, 'to_messages'),<br>\n",
    " (60, 'shared_receipt_with_poi'),<br>\n",
    " (60, 'from_this_person_to_poi'),<br>\n",
    " (60, 'from_poi_to_this_person'),<br>\n",
    " (60, 'from_messages'),<br>\n",
    " (53, 'other'),<br>\n",
    " (51, 'salary'),<br>\n",
    " (51, 'expenses'),<br>\n",
    " (44, 'exercised_stock_options'),<br>\n",
    " (36, 'restricted_stock'),<br>\n",
    " (35, 'email_address'),<br>\n",
    " (21, 'total_payments'),<br>\n",
    " (20, 'total_stock_value')]\n",
    "\n",
    "**Outliers**\n",
    "\n",
    "_When plotted salary and bonus, one dot was outstanding. Looking further, the key of this data was TOTAL, meaning the sum of all other registries. It was cleary an outlier that would interfere negatively on the machine learning procedure. Therefore, this registry was dropped of the dataset to aviod missleading conclusions._\n",
    "\n",
    "![outlier](imgs/outlier.png)\n",
    "\n",
    "_And here is the new plot without the outlier:_\n",
    "\n",
    "![outlier_fix](imgs/outlier_fix.png)\n",
    "\n",
    "_Another procedure used to find any strange key/outlier was looking for a key with only one word or more than 4. Since the keys were the complete name of Enron company employees, a name with only one word or more than 4 would seems unusual (like what happened with TOTAL). For this query, the key 'THE TRAVEL AGENCY IN THE PARK' was found and dropped from the dataset._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "** What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “intelligently select features”, “properly scale features”] **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Selection**\n",
    "\n",
    "*For my final POI identifier, I end up using the following features: 'other', 'exercised_stock_options', 'expenses', 'fraction_to_poi', 'shared_receipt_with_poi', representing 23% of the total features available. To select these features, it was taken 3 steps.*\n",
    "\n",
    "* _**Step 1:**_ _Look for features with high NaN count. The features with high NaN count and low or none POI on not-NaN registries were discarded, since they would aggregate low information to POI identification. Also, since e-mail was a text string it was discarded._\n",
    "\n",
    "* _**Step 2:**_ _Look for features importances on Decision Tree Classifier and features scores on SelectKBest algorithm. This gave me an idea of the most important features._\n",
    "\n",
    "* _**Step 3:**_ *Run several combinations of feature selection and classifier algorithms to assess the best performance. The best match was with decision tree feature importances higher than 0.1.*\n",
    "\n",
    "**Scaling**\n",
    "\n",
    "_On step 3, it was assessed feature scaling (MinMaxScaler) with Support Vector Machine Classifier and GaussianNB. Some combinations were better than without scaling, but the best performance was on decision tree classifier, wich is not affected by scaling. So I choosed to does not use scaling on the final classifier._\n",
    "\n",
    "**Own features**\n",
    "\n",
    "*For this project, I created two new features, a fraction of emails sent to POIs by total emails sent and a fraction of emails received from POIs by total emails received. This could help to narrow how frequent was these to and from POI emails. With just the number of email sent/received of POIs it is not in context on how frequent emailer/receiver this person was. *\n",
    "\n",
    "*This choice proved successfull with high feature importance and score for fraction_to_poi on both decision tree and kbest algorithms*\n",
    "\n",
    "**Feature importances**\n",
    "\n",
    "*On the feature selection step I ran decision tree feature importances and SelectKBest on the available (plus engineered) features. Here's the result:*\n",
    "\n",
    "\n",
    "| feature           | Decision Tree Feature Importances | SelectKBest feature scores |\n",
    "|-------------------|-----------------------------------|----------------------------|\n",
    "| salary            |    0.0423703703704                               |        18.2896840434                    |\n",
    "| to_messages       |     0.0                              |              1.64634112944              |\n",
    "| deferral_payments |      0.0                             |           0.224611274736                 |\n",
    "| total_payments |         0.0674899470899                          |           8.77277773009                 |\n",
    "| exercised_stock_options |           0.255473492355                        |       24.8150797332                     |\n",
    "| bonus |                 0.0                  |           20.7922520472                 |\n",
    "| restricted_stock |          0.0705174291939                         |             9.21281062198               |\n",
    "| shared_receipt_with_poi |        0.118754511278                           |              8.58942073168              |\n",
    "| total_stock_value |             0.0                      |              24.1828986786              |\n",
    "| expenses |                   0.119465329992                |              6.09417331064              |\n",
    "| from_messages |         0.0                          |              0.169700947622              |\n",
    "| other |                  0.190326190476                 |              4.187477507              |\n",
    "| from_this_person_to_poi |         0.0                          |             2.38261210823               |\n",
    "| deferred_income |               0.0                    |                11.4584765793            |\n",
    "| long_term_incentive |           0.0                        |               9.92218601319             |\n",
    "| from_poi_to_this_person |        0.0                           |             5.24344971337               |\n",
    "| fraction_to_poi |              0.135602729245                     |           16.409712548                 |\n",
    "| fraction_from_poi |             0.0                      |               3.12809174816             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "**What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The algorithm with the best result was Decision Tree Classifier. I've assessed also the Gaussian Naive Bayes and Support Vector Machine Classifier.*\n",
    "\n",
    "*The Support Vector Machine did not presented satisfactory results in none of the tested configurations. It was assessed with several feature selections such as None, PCA, MaxMinScaler+PCA, SelectKBest and several algorithm parameters, like C and verbose.*\n",
    "\n",
    "*The Gaussian Naive Bayes had almost as good results as Decision Tree Classifier. It could be a good choice of algorithm due to simplicity, but I tought that recall and precision scores should prevail in the choice.*\n",
    "\n",
    "*Here's the algorithms best scores with the parameter scoring of the GridSearchCV set to 'recall'. Both precision and recall are important results on this POI identifier scenarium and f1 score could balance this goal, but it seems that is more important identify all the persons involved in the scheme, even though some innocent end up classified. It is not such a problem since the goal of this classifier is to identify persons who will later be investigated. To maximize this it is important to minimize false negatives, or high recall scores.*\n",
    "\n",
    "*Another important consideration is that for this scoring was used the test/train split strategy with 30% of the data to test.*\n",
    "\n",
    "| algorithm           | recall | precision |\n",
    "|-------------------|-----------------------------------|----------------------------|\n",
    "| Decision Tree Classifier     |      0.833          |           0.455         |\n",
    "| Gaussian Naive Bayes     |      0.6          |           0.6         |\n",
    "| Support Vector Classifier     |      0.2          |           0.5         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "**What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? What parameters did you tune? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric items: “discuss parameter tuning”, “tune the algorithm”]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Algorithm parameter tuning is the task to try several parameters combinations to train the algorithm. Tuning the parameters is very important and can lead to better prediction results on the same algorithm. On the other hand, parameter tuning can lead to incorrect predictions if it overfits the training data. On this situation, the algorithm looks very good to predict the training data, but on the test data it would miss a lot.*\n",
    "\n",
    "*For my Decision Tree Classifier parameter tuning I used the GridSearchCV function that test all the inputed parameter combinations and present the best result on the selected scoring (recall on my case as talked about on question 3). The parameters considered to this comparison were:*\n",
    "\n",
    "* **min_samples_split:** [**2**, 3, 4, 5, 6]\n",
    "* **max_depht:** [**2**, 3, 4, 5, 6]\n",
    "* **min_samples_leaf:** [**1**, 2, 3]\n",
    "* **criterion:** [**'gini'**, 'entropy']\n",
    "* **class_weight:** [None, **'balanced'**]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queston 5\n",
    "**What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric items: “discuss validation”, “validation strategy”]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Validation is the action of assessing if your algorithm is performing well, meaning that it can accurately predict results for new datapoints. To do this, the whole dataset is splitted in test and training. The training part is used to train the machine learning algorithm and the test part to compare what the algorithm predicted with the actual result. This process is crucial to machine learning because it can give estimate performance on an indepentent dataset and serves as check on overfitting. A classic mistake is to use the same dataset to train and test the results. This approach will most likely show biased results, since the algorithm could do well only on the data wich it learned, but the purpose of the algorithm is to predict accurately new and unknown datapoints.*\n",
    "\n",
    "*On my analysis I used the train test sklearn split function to split 30% of the data to test and 70% to train. Also, I've used the kfold function to do cross-validation. On this approach, the dataset was divided in 4 parts (25%) and all these parts are combined in test and train datasets. This is a very robust way to assess the algorithm performance, since all the data is used both for training and testing.*\n",
    "\n",
    "*And lastly, the test_classifier function from the original project code was used to check the algorithm performance. This function uses the StratifiedShuffleSplit as a cross-validator to split the data in train/test sets.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "**Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The most common evaluation metric is accuracy. This measure how frequent the algorithm hit the correct result. It is very important metric, but it can be inadequate depending on the dataset shape. On our POI analysis, only 18 of 146 datapoints were POI. Because of this, almost any algorithm will have high accuracy since only getting correct the non POIs would get good results. But our goal is actually the opposite, to identify the low frequent POIs.*\n",
    "\n",
    "*For this task, there are two metrics that would indicate better than accuracy how the algorithm was performing. These metrics are precision and recall. Precision is: if all the items that are labeled positive, how many were correctly ranked? And recall is: Of all cases that should be labeled positive, how many were correctly classified?*\n",
    "\n",
    "*In other words, recall measures how good the algorithm is flag POIs when it is present on the test data. And precision measures how good the algorithm is to when flag POIs, they are a real POI and not a false alarm.*\n",
    "\n",
    "*My algorithm scored 0.341 for precision and 0.684 for recall. This means that it will likely tag real POIs, but sometimes non-POIs will be flagged. On almost 34% of the times that the algorithm identify a POI, the person was actually a POI (precision), and on 68% that a person is a POI, the algorithm identify it (recall).*\n",
    "\n",
    "*For this project, I think that recall is a better metric than precision because if the person was really a POI it should be flagged to be investigated, even if the cost was to investigate some innocent people. Thats why I choose recall over precision on my algorithm selection.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project References\n",
    "* [features importances on decision tree](http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html)\n",
    "* [features importances on decision tree (2)](https://datascience.stackexchange.com/questions/16693/interpreting-decision-tree-in-context-of-feature-importances)\n",
    "* [recall score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)\n",
    "* [Pipeline function](http://scikit-learn.org/stable/modules/pipeline.html)\n",
    "* [GridSearchCV function](http://scikit-learn.org/stable/modules/grid_search.html#grid-search)\n",
    "* [Pipeline and GridSearchCV](http://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html)\n",
    "* [scoring parameter on GridSearchCV](https://stackoverflow.com/questions/32889929/gridsearchcv-scoring-parameter-using-scoring-f1-or-scoring-none-by-default-u)\n",
    "* [classification report](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)\n",
    "* [feature scaling and normalization](http://sebastianraschka.com/Articles/2014_about_feature_scaling.html#about-min-max-scaling)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
